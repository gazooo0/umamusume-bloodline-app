import streamlit as st
import pandas as pd
import unicodedata
import re
import time
from bs4 import BeautifulSoup
import requests

# === è¨­å®š ===
HEADERS = {"User-Agent": "Mozilla/5.0"}

# === ã‚¦ãƒå¨˜è¡€çµ± ===
umamusume_bloodlines = {
    "ã‚¢ãƒ¼ãƒ¢ãƒ³ãƒ‰ã‚¢ã‚¤", "ã‚¢ã‚¤ãƒã‚¹ãƒ•ã‚¦ã‚¸ãƒ³", "ã‚¢ã‚°ãƒã‚¹ã‚¿ã‚­ã‚ªãƒ³", "ã‚¢ã‚°ãƒã‚¹ãƒ‡ã‚¸ã‚¿ãƒ«", "ã‚¢ã‚¹ãƒˆãƒ³ãƒãƒ¼ãƒãƒ£ãƒ³",
    "ã‚¢ãƒ‰ãƒã‚¤ãƒ¤ãƒ™ã‚¬", "ã‚¢ãƒ‰ãƒã‚¤ãƒ¤ãƒœã‚¹", "ã‚¢ãƒ‰ãƒã‚¤ãƒ¤ãƒ ãƒ¼ãƒ³", "ã‚¤ãƒŠãƒªãƒ¯ãƒ³", "ã‚¦ã‚¤ãƒ‹ãƒ³ã‚°ãƒã‚±ãƒƒãƒˆ",
    "ãƒ´ã‚£ãƒ«ã‚·ãƒ¼ãƒŠ", "ã‚¦ã‚ªãƒƒã‚«", "ã‚¨ã‚¢ã‚°ãƒ«ãƒ¼ãƒ´", "ã‚¨ã‚¤ã‚·ãƒ³ãƒ•ãƒ©ãƒƒã‚·ãƒ¥", "ã‚¨ã‚¤ã‚·ãƒ³ãƒ’ã‚«ãƒª", "ã‚¨ã‚¤ã‚·ãƒ³ãƒ¯ã‚·ãƒ³ãƒˆãƒ³",
    "ã‚¨ãƒ«ã‚³ãƒ³ãƒ‰ãƒ«ãƒ‘ã‚µãƒ¼", "ã‚ªã‚°ãƒªã‚­ãƒ£ãƒƒãƒ—", "ã‚«ãƒ¬ãƒ³ãƒãƒ£ãƒ³", "ã‚«ãƒ¯ã‚«ãƒŸãƒ—ãƒªãƒ³ã‚»ã‚¹", "ã‚«ãƒ¯ãƒ’ãƒ¡", "ã‚«ãƒ¯ãƒ’ãƒ¡", "ã‚­ãƒ³ã‚°ã‚«ãƒ¡ãƒãƒ¡ãƒ",
    "ã‚°ãƒ©ã‚¹ãƒ¯ãƒ³ãƒ€ãƒ¼", "ã‚±ã‚¤ã‚¨ã‚¹ãƒŸãƒ©ã‚¯ãƒ«", "ã‚´ãƒ¼ãƒ«ãƒ‰ã‚·ãƒƒãƒ—", "ã‚µã‚¤ãƒ¬ãƒ³ã‚¹ã‚¹ã‚ºã‚«", "ã‚µã‚¯ãƒ©ãƒã‚¯ã‚·ãƒ³ã‚ªãƒ¼", "ã‚µãƒˆãƒã‚¯ãƒ©ã‚¦ãƒ³",
    "ã‚µãƒˆãƒãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰", "ã‚·ãƒ¼ã‚­ãƒ³ã‚°ã‚¶ãƒ‘ãƒ¼ãƒ«", "ã‚·ãƒ³ãƒœãƒªã‚¯ãƒªã‚¹ã‚¨ã‚¹", "ã‚·ãƒ³ãƒœãƒªãƒ«ãƒ‰ãƒ«ãƒ•", "ã‚¹ã‚¤ãƒ¼ãƒ—ãƒˆã‚¦ã‚·ãƒ§ã‚¦", "ã‚¹ãƒšã‚·ãƒ£ãƒ«ã‚¦ã‚£ãƒ¼ã‚¯",
    "ã‚»ã‚¤ã‚¦ãƒ³ã‚¹ã‚«ã‚¤", "ã‚¼ãƒ³ãƒãƒ­ãƒ–ãƒ­ã‚¤", "ã‚¿ã‚¤ã‚­ã‚·ãƒ£ãƒˆãƒ«", "ã‚¿ãƒ‹ãƒã‚®ãƒ ãƒ¬ãƒƒãƒˆ", "ã‚¿ãƒƒãƒ—ãƒ€ãƒ³ã‚¹ã‚·ãƒãƒ¼", "ãƒ€ã‚¤ãƒ¯ã‚¹ã‚«ãƒ¼ãƒ¬ãƒƒãƒˆ",
    "ãƒ€ã‚¤ãƒ¯ãƒ¡ã‚¸ãƒ£ãƒ¼", "ãƒ€ãƒ³ãƒ„ãƒ•ãƒ¬ãƒ¼ãƒ ", "ãƒã‚§ãƒªãƒ¼ã‚³ã‚¦ãƒãƒ³", "ãƒ„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒœ", "ãƒ†ã‚¤ã‚¨ãƒ ã‚ªãƒšãƒ©ã‚ªãƒ¼", "ãƒ†ã‚¤ã‚¨ãƒ ãƒ—ãƒªã‚­ãƒ¥ã‚¢",
    "ãƒŠã‚¤ã‚¹ãƒã‚¤ãƒãƒ£", "ãƒŠãƒªã‚¿ã‚¿ã‚¤ã‚·ãƒ³", "ãƒŠãƒªã‚¿ãƒ–ãƒ©ã‚¤ã‚¢ãƒ³", "ãƒ‹ã‚·ãƒãƒ•ãƒ©ãƒ¯ãƒ¼", "ãƒ‹ãƒ›ãƒ³ãƒ”ãƒ­ã‚¦ã‚¤ãƒŠãƒ¼", "ãƒã‚ªãƒ¦ãƒ‹ãƒ´ã‚¡ãƒ¼ã‚¹",
    "ãƒãƒ¼ã‚¶ãƒ³ãƒ†ãƒ¼ã‚¹ãƒˆ", "ãƒãƒ¼ã‚¹ãƒ•ãƒ©ã‚¤ãƒˆ", "ãƒãƒ¼ãƒ„ã‚¯ãƒ©ã‚¤", "ãƒãƒ³ãƒ–ãƒ¼ãƒ¡ãƒ¢ãƒªãƒ¼", "ãƒ’ã‚·ã‚¢ãƒã‚¾ãƒ³", "ãƒ“ãƒ¯ãƒãƒ¤ãƒ’ãƒ‡",
    "ãƒ•ã‚¡ã‚¤ãƒ³ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³", "ãƒ•ã‚¸ã‚­ã‚»ã‚­", "ãƒ•ãƒ©ãƒ¯ãƒ¼ãƒ‘ãƒ¼ã‚¯", "ãƒ–ã‚¨ãƒŠãƒ“ã‚¹ã‚¿", "ãƒ›ãƒƒã‚³ãƒ¼ã‚¿ãƒ«ãƒã‚¨", "ãƒãƒ¼ãƒ™ãƒ©ã‚¹ã‚µãƒ³ãƒ‡ãƒ¼",
    "ãƒãƒã‚«ãƒãƒ•ã‚¯ã‚­ã‚¿ãƒ«", "ãƒãƒ¤ãƒãƒˆãƒƒãƒ—ã‚¬ãƒ³", "ãƒŸã‚¹ã‚¿ãƒ¼ã‚·ãƒ¼ãƒ“ãƒ¼", "ãƒŸãƒ›ãƒãƒ–ãƒ«ãƒœãƒ³", "ãƒ¡ã‚¤ã‚·ãƒ§ã‚¦ãƒ‰ãƒˆã‚¦", "ãƒ¡ã‚¸ãƒ­ã‚¢ãƒ«ãƒ€ãƒ³",
    "ãƒ¡ã‚¸ãƒ­ãƒ‰ãƒ¼ãƒ™ãƒ«", "ãƒ¡ã‚¸ãƒ­ãƒ‘ãƒ¼ãƒãƒ¼", "ãƒ¡ã‚¸ãƒ­ãƒãƒƒã‚¯ã‚¤ãƒ¼ãƒ³", "ãƒ©ã‚¤ã‚¹ã‚·ãƒ£ãƒ¯ãƒ¼", "ãƒ©ãƒƒã‚­ãƒ¼ãƒ©ã‚¤ãƒ©ãƒƒã‚¯", "ãƒ­ã‚¤ã‚¹ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ã‚¹",
    "ãƒ¯ãƒ³ãƒ€ãƒ¼ã‚¢ã‚­ãƒ¥ãƒ¼ãƒˆ"
}
normalized_umamusume = {unicodedata.normalize("NFKC", n).strip().lower() for n in umamusume_bloodlines}

def generate_position_labels():
    def dfs(pos, depth, max_depth):
        if depth > max_depth: return []
        result = [pos]
        result += dfs(pos + "çˆ¶", depth + 1, max_depth)
        result += dfs(pos + "æ¯", depth + 1, max_depth)
        return result
    return dfs("", 0, 5)[1:]
POSITION_LABELS = generate_position_labels()

def get_horse_links(race_id):
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    res = requests.get(url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    horse_links = {}
    tables = soup.find_all("table", class_="RaceTable01")
    for table in tables:
        for a in table.find_all("a", href=True):
            if "/horse/" in a["href"]:
                name = a.get_text(strip=True)
                full_url = "https://db.netkeiba.com" + a["href"]
                if len(name) >= 2 and name not in horse_links:
                    horse_links[name] = full_url
    return horse_links

def get_pedigree_with_positions(horse_url):
    horse_id = horse_url.rstrip("/").split("/")[-1]
    ped_url = f"https://db.netkeiba.com/horse/ped/{horse_id}/"
    res = requests.get(ped_url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    table = soup.find("table", class_="blood_table")
    if not table:
        return {}
    names = {}
    td_list = table.find_all("td")
    for i, td in enumerate(td_list[:len(POSITION_LABELS)]):
        label = POSITION_LABELS[i]
        a = td.find("a")
        if a and a.text.strip():
            names[label] = a.text.strip()
    return names

def match_umamusume(pedigree_dict):
    return [f"ã€{pos}ã€‘{name}" for pos, name in pedigree_dict.items()
            if unicodedata.normalize("NFKC", name).strip().lower() in normalized_umamusume]

def analyze_race(race_id):
    horse_links = get_horse_links(race_id)
    st.text(f"ğŸ å‡ºèµ°é¦¬æ•°: {len(horse_links)}é ­")

    result = []
    for idx, (name, link) in enumerate(horse_links.items(), 1):
        with st.spinner(f"{idx}é ­ç›®ï¼š{name} ã‚’ç…§åˆä¸­..."):
            try:
                pedigree = get_pedigree_with_positions(link)
                matches = match_umamusume(pedigree)
                result.append({
                    "é¦¬å": name,
                    "è©²å½“è¡€çµ±æ•°": len(matches),
                    "ã‚¦ãƒå¨˜è¡€çµ±": "\n".join(matches)
                })
            except Exception as e:
                result.append({
                    "é¦¬å": name,
                    "è©²å½“è¡€çµ±æ•°": "å–å¾—å¤±æ•—",
                    "ã‚¦ãƒå¨˜è¡€çµ±": str(e)
                })
            time.sleep(1.5)
    return result

# === UI ===
st.title("ğŸ‡ã‚¦ãƒå¨˜è¡€çµ±ã‚µãƒ¼ãƒ")
st.markdown("### ï¼ˆæœ€æ–°1ã‹æœˆé–“å¯¾å¿œï¼‰")

schedule_df = pd.read_csv("jra_2025_keibabook_schedule.csv")

# âœ… æ—¥ä»˜æ•´å½¢ã®ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼ˆã“ã“ï¼‰
schedule_df["æ—¥ä»˜"] = pd.to_datetime(
    schedule_df["å¹´"].astype(str) + "/" + schedule_df["æœˆæ—¥(æ›œæ—¥)"].str.extract(r"(\d{2}/\d{2})")[0],
    format="%Y/%m/%d"
)

today = pd.Timestamp.today()
past_31 = today - pd.Timedelta(days=31)
schedule_df = schedule_df[schedule_df["æ—¥ä»˜"].between(past_31, today)]

# ğŸ“… æ—¥ä»˜é¸æŠï¼ˆæœ€æ–°ãŒä¸Šï¼‰
dates = sorted(schedule_df["æ—¥ä»˜"].dt.strftime("%Y-%m-%d").unique(), reverse=True)
st.markdown("### ğŸ“… ç«¶é¦¬é–‹å‚¬æ—¥ã‚’é¸æŠ")
selected_date = st.selectbox("ï¼ˆéå»31æ—¥ã¾ã§é¡ã‚Œã¾ã™ã€‚ï¼‰", dates)
data_filtered = schedule_df[schedule_df["æ—¥ä»˜"].dt.strftime("%Y-%m-%d") == selected_date]

# ğŸ‡ ç«¶é¦¬å ´é¸æŠï¼ˆãƒœã‚¿ãƒ³å½¢å¼ï¼‰
st.markdown("### ğŸŸï¸ ç«¶é¦¬å ´ã‚’é¸æŠã€‚")
place_codes = {"æœ­å¹Œ": "01", "å‡½é¤¨": "02", "ç¦å³¶": "03", "æ–°æ½Ÿ": "04", "æ±äº¬": "05",
               "ä¸­å±±": "06", "ä¸­äº¬": "07", "äº¬éƒ½": "08", "é˜ªç¥": "09", "å°å€‰": "10"}
available_places = sorted(data_filtered["ç«¶é¦¬å ´"].unique())
cols = st.columns(5)
if "place" not in st.session_state:
    st.session_state.place = None
for i, p in enumerate(available_places):
    if cols[i % 5].button(p):
        st.session_state.place = p
place = st.session_state.place
if not place:
    st.stop()

# ğŸ ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’é¸æŠï¼ˆãƒ—ãƒ«ãƒ€ã‚¦ãƒ³å½¢å¼ï¼‰
st.markdown("### ğŸ ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’é¸æŠã€‚")
race_num_int = st.selectbox("ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’é¸ã‚“ã§ãã ã•ã„", list(range(1, 13)), format_func=lambda x: f"{x}R")

if not race_num_int:
    st.stop()

selected_row = data_filtered[data_filtered["ç«¶é¦¬å ´"] == place].iloc[0]
jj = place_codes[place]
kk = f"{int(selected_row['é–‹å‚¬å›']):02d}"
dd = f"{int(selected_row['æ—¥ç›®']):02d}"
race_id = f"{selected_row['å¹´']}{jj}{kk}{dd}{race_num_int:02d}"
st.markdown(f"ğŸ”¢ **race_id**: `{race_id}`")

# å®Ÿè¡Œãƒœã‚¿ãƒ³
if st.button("ğŸ‡ ã‚¦ãƒå¨˜è¡€çµ±ã®ãŠé¦¬ã•ã‚“ã‚µãƒ¼ãƒé–‹å§‹ï¼"):
    with st.spinner("ç…§åˆä¸­..."):
        results = analyze_race(race_id)
        st.success("ç…§åˆå®Œäº†ï¼")

        if not results:
            st.warning("å‡ºèµ°é¦¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.markdown("### ğŸ§¬ ã‚¦ãƒå¨˜è¡€çµ±ã‚µãƒ¼ãƒçµæœ")
            for idx, row in enumerate(results):
                st.markdown(f"""
<div style='font-size:20px; font-weight:bold;'>{idx+1}. {row['é¦¬å']}</div>

è©²å½“è¡€çµ±æ•°ï¼š{row['è©²å½“è¡€çµ±æ•°']}  
{row['ã‚¦ãƒå¨˜è¡€çµ±']}
""", unsafe_allow_html=True)
                if idx < len(results) - 1:
                    st.markdown("<hr style='border:1px solid #ccc;'>", unsafe_allow_html=True)
