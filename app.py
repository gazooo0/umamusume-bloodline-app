import streamlit as st
import pandas as pd
import unicodedata
import re
import time
import os
import json
import base64
from bs4 import BeautifulSoup
import requests
import gspread
from google.oauth2.service_account import Credentials

# === Google Sheetsè¨­å®š ===
SHEET_ID = "1wMkpbOvqveVBkJSR85mpZcnKThYSEmusmsl710SaRKw"
SHEET_NAME = "cache"
SCOPE = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]

# èªè¨¼æƒ…å ±ã®èª­ã¿è¾¼ã¿
if "GOOGLE_SERVICE_JSON" in os.environ:
    service_account_info = json.loads(os.environ["GOOGLE_SERVICE_JSON"])
else:
    with open("service_account.json", "r", encoding="utf-8") as f:
        service_account_info = json.load(f)

credentials = Credentials.from_service_account_info(service_account_info, scopes=SCOPE)
gc = gspread.authorize(credentials)
sheet = gc.open_by_key(SHEET_ID).worksheet(SHEET_NAME)

# GitHub Actions ç”¨ï¼šSecrets ã‹ã‚‰ service_account.json ã‚’å†ç”Ÿæˆ
if "GOOGLE_CREDS_JSON" in os.environ:
    decoded = base64.b64decode(os.environ["GOOGLE_CREDS_JSON"])
    with open("service_account.json", "wb") as f:
        f.write(decoded)

# === è¨­å®š ===
HEADERS = {"User-Agent": "Mozilla/5.0"}

# === ã‚¦ãƒå¨˜è¡€çµ±ãƒ‡ãƒ¼ã‚¿ ===
umamusume_df = pd.read_csv("umamusume.csv")
image_dict = dict(zip(umamusume_df["kettou"], umamusume_df["url"]))
umamusume_bloodlines = set(umamusume_df["kettou"].dropna().astype(str))
normalized_umamusume = {unicodedata.normalize("NFKC", n).strip().lower() for n in umamusume_bloodlines}

# === è¡€çµ±ä½ç½®ãƒ©ãƒ™ãƒ« ===
def generate_position_labels():
    def dfs(pos, depth, max_depth):
        if depth > max_depth:
            return []
        result = [pos]
        result += dfs(pos + "çˆ¶", depth + 1, max_depth)
        result += dfs(pos + "æ¯", depth + 1, max_depth)
        return result
    return dfs("", 0, 5)[1:]
POSITION_LABELS = generate_position_labels()

# === å‡ºèµ°é¦¬ãƒªãƒ³ã‚¯å–å¾— ===
def get_horse_links(race_id):
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    res = requests.get(url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    horse_links = {}
    tables = soup.find_all("table", class_="RaceTable01")
    for table in tables:
        for a in table.find_all("a", href=True):
            if "/horse/" in a["href"]:
                name = a.get_text(strip=True)
                full_url = "https://db.netkeiba.com" + a["href"]
                if len(name) >= 2 and name not in horse_links:
                    horse_links[name] = full_url
    return horse_links

# === è¡€çµ±å–å¾— ===
def get_pedigree_with_positions(horse_url):
    horse_id = horse_url.rstrip("/").split("/")[-1]
    ped_url = f"https://db.netkeiba.com/horse/ped/{horse_id}/"
    res = requests.get(ped_url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    table = soup.find("table", class_="blood_table")
    if not table:
        return {}
    names = {}
    td_list = table.find_all("td")
    for i, td in enumerate(td_list[:len(POSITION_LABELS)]):
        label = POSITION_LABELS[i]
        a = td.find("a")
        if a and a.text.strip():
            names[label] = a.text.strip()
    return names

# === ç…§åˆå‡¦ç† ===
def match_umamusume(pedigree_dict):
    matched = []
    for pos, name in pedigree_dict.items():
        key = unicodedata.normalize("NFKC", name).strip().lower()
        if key in normalized_umamusume:
            img_url = image_dict.get(name, "")
            label = pos.replace("ã€‘", "").replace("ã€", "")  # ä¾‹ï¼šæ¯çˆ¶
            if img_url:
                matched.append(
                    f"""
<div style='display: flex; align-items: center; margin-bottom: 8px;'>
  <img src="{img_url}" width="80" style="margin-right: 12px; border-radius: 4px;">
  <div style="line-height: 1;">
    <div style="font-size: 0.9em; font-weight: bold;">{label}</div>
    <div style="font-size: 0.95em;">{name}</div>
  </div>
</div>
""")
            else:
                matched.append(f"{label}<br>{name}")
    return matched

# === Google Sheets ã‚­ãƒ£ãƒƒã‚·ãƒ¥ ===
def load_cached_result(race_id):
    try:
        records = sheet.get_all_records()
        matched = [r for r in records if str(r.get("race_id")) == str(race_id)]
        if matched:
            return pd.DataFrame(matched)
    except Exception as e:
        st.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    return None

def save_cached_result(race_id, df):
    df["race_id"] = race_id
    try:
        all_values = sheet.get_all_values()
        headers = all_values[0]
        data_rows = all_values[1:]
        if "race_id" in headers:
            race_id_col_idx = headers.index("race_id")
        else:
            st.error("Google Sheets ã« 'race_id' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        rows_to_delete = [i + 2 for i, row in enumerate(data_rows)
                          if len(row) > race_id_col_idx and row[race_id_col_idx] == race_id]
        if rows_to_delete:
            requests = [{"deleteDimension": {
                "range": {
                    "sheetId": sheet.id,
                    "dimension": "ROWS",
                    "startIndex": row - 1,
                    "endIndex": row
                }}} for row in sorted(rows_to_delete, reverse=True)]
            sheet.spreadsheet.batch_update({"requests": requests})
            time.sleep(1.0)
            df = df[["é¦¬å", "è©²å½“æ•°", "è©²å½“ç®‡æ‰€", "race_id"]]
        sheet.append_rows(df.values.tolist(), value_input_option="USER_ENTERED")
    except Exception as e:
        st.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# === Streamlit UI ===
st.title("ã‚¦ãƒå¨˜è¡€çµ±ğŸã‚µãƒ¼ãƒ")
schedule_df = pd.read_csv("jra_2025_keibabook_schedule.csv")
schedule_df["æ—¥ä»˜"] = pd.to_datetime(
    schedule_df["å¹´"].astype(str) + "/" + schedule_df["æœˆæ—¥(æ›œæ—¥)"].str.extract(r"(\d{2}/\d{2})")[0],
    format="%Y/%m/%d")

today = pd.Timestamp.today()
past_31 = today - pd.Timedelta(days=31)
future_7 = today + pd.Timedelta(days=7)
schedule_df = schedule_df[schedule_df["æ—¥ä»˜"].between(past_31, future_7)]

dates = sorted(schedule_df["æ—¥ä»˜"].dt.strftime("%Y-%m-%d").unique(), reverse=True)
st.markdown("### ğŸ“… é–‹å‚¬æ—¥ã‚’é¸æŠ")
selected_date = st.selectbox("ï¼ˆéå»31æ—¥ã€œæœªæ¥7æ—¥ï¼‰", dates)
data_filtered = schedule_df[schedule_df["æ—¥ä»˜"].dt.strftime("%Y-%m-%d") == selected_date]

st.markdown("### ğŸŒ ç«¶é¦¬å ´ã‚’é¸æŠ")
place_codes = {"æœ­å¹Œ": "01", "å‡½é¤¨": "02", "ç¦å³¶": "03", "æ–°æ½Ÿ": "04", "æ±äº¬": "05",
               "ä¸­å±±": "06", "ä¸­äº¬": "07", "äº¬éƒ½": "08", "é˜ªç¥": "09", "å°å€‰": "10"}
available_places = sorted(data_filtered["ç«¶é¦¬å ´"].unique())
cols = st.columns(5)
if "place" not in st.session_state:
    st.session_state.place = None
for i, p in enumerate(available_places):
    if cols[i % 5].button(p):
        st.session_state.place = p
place = st.session_state.place
if not place:
    st.stop()

st.markdown("### ğŸ ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’é¸æŠ")
race_num_int = st.selectbox("ãƒ¬ãƒ¼ã‚¹ç•ªå·", list(range(1, 13)), format_func=lambda x: f"{x}R")
st.markdown("""
<div style='line-height: 1.5; font-size: 0,8em; color: gray;'>
<b>â— ã€Œé‡è³ã€(Gâ…¢ãƒ»Gâ…¡ãƒ»Gâ… )ã¯ãƒ¡ã‚¤ãƒ³ãƒ¬ãƒ¼ã‚¹ã¨ã—ã¦11Rã«è¡Œã‚ã‚Œã¾ã™ã€‚<br>
â—ã€€é¿æš‘æœŸé–“ï¼ˆæ–°æ½Ÿãƒ»ä¸­äº¬ï¼š7/26(åœŸ)ï½8/17(æ—¥)ï¼‰ã®ãƒ¡ã‚¤ãƒ³ã¯7Rã§ã™ã€‚</b><br><br>
</div>
""", unsafe_allow_html=True)

filtered = data_filtered[data_filtered["ç«¶é¦¬å ´"] == place]
if filtered.empty:
    st.warning(f"âš  {place} ã®æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    st.stop()
selected_row = filtered.iloc[0]

# race_id ã‚’ç”Ÿæˆ
jj = place_codes.get(place, "")
kk = f"{int(selected_row['é–‹å‚¬å›']):02d}"
dd = f"{int(selected_row['æ—¥ç›®']):02d}"
race_id = f"{selected_row['å¹´']}{jj}{kk}{dd}{race_num_int:02d}"

if not race_num_int:
    st.stop()

st.markdown("### ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åˆ©ç”¨")

st.markdown("""
<div style='line-height: 1.5; font-size: 0.8em; color: white;'>
<b>è² è·è»½æ¸›ã®ãŸã‚ã€åŸºæœ¬ã¯ã€Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã™ã‚‹ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚</b><br>
ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ãŒãªã„å ´åˆã¯è‡ªå‹•ã§æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã‚ˆã†ã«å‹•ãã¾ã™ã€‚<br>
è‡ªå‹•å–å¾—ã®å ´åˆã€é †ç•ªã«è¡¨ç¤ºã•ã‚Œã‚‹ã®ã§ãŠå¾…ã¡ãã ã•ã„ã€‚<br><br></div>
""", unsafe_allow_html=True)

# âœ… st.radio ã®å¾Œã«å€¤ã‚’å–å¾—ã—ã¦ã‹ã‚‰ä½¿ã†
use_cache = st.radio("äº‹å‰ã«ä¿å­˜ã•ã‚ŒãŸæƒ…å ±ã‚’â€¦", ["åˆ©ç”¨ã™ã‚‹", "æœ€æ–°æƒ…å ±ã‚’å–å¾—ã™ã‚‹"], horizontal=True)
use_cache_bool = use_cache == "åˆ©ç”¨ã™ã‚‹"

st.markdown("""
<div style='line-height: 1.5; font-size: 0.8em; color: gray;'>
  ã€å‚è€ƒã€‘<br>
  ä¸‹è¨˜ã®é€šã‚Šã€æƒ…å ±æ›´æ–°ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã«ã‚ˆã£ã¦ã¯å¤ã„æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚<br>
  ã‚ˆã£ã¦ã€å¤ã„æƒ…å ±ã‚’è¡¨ç¤ºã—ã¦ã—ã¾ã†å ´åˆã«é™ã‚Šã€ã€Œæœ€æ–°æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€ã‚’é¸æŠã—ãŸä¸Šã§ã€ãƒ¬ãƒ¼ã‚¹ã«ã¤ã1å›ã ã‘æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚<br>
ã€€  â– ã€€ç‰¹åˆ¥ç™»éŒ²ï¼šå‰é€±æ—¥æ›œæ—¥ã®18æ™‚å‰å¾Œï¼ˆå‰ã€…é€±ã®ç‰¹åˆ¥ç™»éŒ²ã«ã¯æœªå¯¾å¿œï¼‰<br>
 ã€€ â– ã€€å‡ºèµ°æƒ³å®šï¼šï¼ˆæ°´ï¼‰20æ™‚å‰å¾Œï¼ˆç‰¹åˆ¥ç™»éŒ²ä»¥å¤–ã®ãƒ¬ãƒ¼ã‚¹ã§ã¯æœ€åˆã®3é ­ã®ã¿è¡¨ç¤ºï¼‰<br>
  ã€€â– ã€€å‡ºèµ°ç¢ºå®šï¼šï¼ˆæœ¨ï¼‰19æ™‚å‰å¾Œï¼ˆç‰¹åˆ¥ç™»éŒ²ä»¥å¤–ã®ãƒ¬ãƒ¼ã‚¹ã§ã‚‚å…¨é ­è¡¨ç¤ºå¯èƒ½â€»é¦¬åé †ï¼‰<br>
  ã€€â– ã€€æ é †ç¢ºå®šï¼šãƒ¬ãƒ¼ã‚¹å‰æ—¥ã®11æ™‚å‰å¾Œï¼ˆæ é †ã§å‡ºåŠ›å¯èƒ½ï¼‰<br><br>
</div>
""", unsafe_allow_html=True)

st.markdown("### ğŸ‘‡ æ¤œç´¢ã‚’å®Ÿè¡Œ")
if st.button("ğŸ” ã‚¦ãƒå¨˜è¡€çµ±ã‚µãƒ¼ãƒé–‹å§‹"):
    st.session_state.search_state = {
        "race_id": race_id,
        "use_cache": use_cache_bool,
        "triggered": True,
    }

search_state = st.session_state.get("search_state", {})
if search_state.get("triggered") and search_state.get("race_id") == race_id:
    cached_df = load_cached_result(race_id) if search_state.get("use_cache") else None

    if cached_df is not None:
        st.success(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ {len(cached_df)}é ­ã‚’è¡¨ç¤º")
        for idx, row in cached_df.iterrows():
            st.markdown(f"""
<div style='font-size:20px; font-weight:bold;'>{idx + 1}. {row['é¦¬å']}</div>
è©²å½“è¡€çµ±æ•°ï¼š{row['è©²å½“æ•°']}<br>
{row['è©²å½“ç®‡æ‰€']}
""", unsafe_allow_html=True)
            st.markdown("---")
    else:
        horse_links = get_horse_links(race_id)
        st.markdown(f"ğŸ‡ å‡ºèµ°é¦¬æ•°: {len(horse_links)}é ­")
        result_rows = []
        for idx, (name, link) in enumerate(horse_links.items(), 1):
            with st.spinner(f"{idx}é ­ç›®ï¼š{name} ã‚’ç…§åˆä¸­..."):
                try:
                    pedigree = get_pedigree_with_positions(link)
                    matches = match_umamusume(pedigree)
                    st.markdown(f"""
<div style='font-size:20px; font-weight:bold;'>{idx}. {name}</div>
è©²å½“è¡€çµ±æ•°ï¼š{len(matches)}<br>
{'<br>'.join(matches) if matches else 'è©²å½“ãªã—'}
""", unsafe_allow_html=True)
                    result_rows.append({
                        "é¦¬å": name,
                        "è©²å½“æ•°": len(matches),
                        "è©²å½“ç®‡æ‰€": '<br>'.join(matches) if matches else "è©²å½“ãªã—"
                    })
                except Exception as e:
                    st.error(f"{name} ã®ç…§åˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            st.markdown("---")
            time.sleep(1.2)
        if result_rows:
            df = pd.DataFrame(result_rows)
            save_cached_result(race_id, df)
