import streamlit as st
import pandas as pd
import unicodedata
import re
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import requests

# === è¨­å®š ===
CHROMEDRIVER_PATH = "D:/PythonK/chromedriver.exe"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# === ã‚¦ãƒå¨˜è¡€çµ±ãƒªã‚¹ãƒˆ ===
umamusume_bloodlines = {
    "ã‚¢ã‚°ãƒã‚¹ã‚¿ã‚­ã‚ªãƒ³", "ã‚¢ã‚°ãƒã‚¹ãƒ‡ã‚¸ã‚¿ãƒ«", "ã‚¢ãƒ‰ãƒã‚¤ãƒ¤ãƒ™ã‚¬", "ã‚¤ãƒŠãƒªãƒ¯ãƒ³", "ã‚¦ã‚ªãƒƒã‚«", "ã‚¨ã‚¢ã‚°ãƒ«ãƒ¼ãƒ´",
    "ã‚¨ãƒ«ã‚³ãƒ³ãƒ‰ãƒ«ãƒ‘ã‚µãƒ¼", "ã‚«ãƒ¬ãƒ³ãƒãƒ£ãƒ³", "ã‚«ãƒ¯ã‚«ãƒŸãƒ—ãƒªãƒ³ã‚»ã‚¹", "ã‚­ã‚¿ã‚µãƒ³ãƒ–ãƒ©ãƒƒã‚¯", "ã‚­ãƒ³ã‚°ãƒ˜ã‚¤ãƒ­ãƒ¼",
    "ã‚°ãƒ©ã‚¹ãƒ¯ãƒ³ãƒ€ãƒ¼", "ã‚´ãƒ¼ãƒ«ãƒ‰ã‚·ãƒƒãƒ—", "ã‚µã‚¤ãƒ¬ãƒ³ã‚¹ã‚¹ã‚ºã‚«", "ã‚µã‚¯ãƒ©ãƒã‚¯ã‚·ãƒ³ã‚ªãƒ¼", "ã‚µãƒˆãƒãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰",
    "ã‚·ãƒ¼ã‚­ãƒ³ã‚°ã‚¶ãƒ‘ãƒ¼ãƒ«", "ã‚·ãƒ³ãƒœãƒªãƒ«ãƒ‰ãƒ«ãƒ•", "ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¯ãƒªãƒ¼ã‚¯", "ã‚¹ãƒšã‚·ãƒ£ãƒ«ã‚¦ã‚£ãƒ¼ã‚¯", "ã‚»ã‚¤ã‚¦ãƒ³ã‚¹ã‚«ã‚¤",
    "ã‚¼ãƒ³ãƒãƒ­ãƒ–ãƒ­ã‚¤", "ãƒ€ã‚¤ã‚¿ã‚¯ãƒ˜ãƒªã‚ªã‚¹", "ãƒ€ã‚¤ãƒ¯ã‚¹ã‚«ãƒ¼ãƒ¬ãƒƒãƒˆ", "ã‚¿ãƒãƒ¢ã‚¯ãƒ­ã‚¹", "ãƒ„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒœ",
    "ãƒ†ã‚¤ã‚¨ãƒ ã‚ªãƒšãƒ©ã‚ªãƒ¼", "ãƒŠã‚¤ã‚¹ãƒã‚¤ãƒãƒ£", "ãƒŠãƒªã‚¿ã‚¿ã‚¤ã‚·ãƒ³", "ãƒŠãƒªã‚¿ãƒ–ãƒ©ã‚¤ã‚¢ãƒ³", "ãƒ‹ã‚·ãƒãƒ•ãƒ©ãƒ¯ãƒ¼",
    "ãƒãƒ«ã‚¦ãƒ©ãƒ©", "ãƒãƒ³ãƒ–ãƒ¼ãƒ¡ãƒ¢ãƒªãƒ¼", "ãƒ“ã‚³ãƒ¼ãƒšã‚¬ã‚µã‚¹", "ãƒ’ã‚·ã‚¢ã‚±ãƒœãƒ", "ãƒ’ã‚·ã‚¢ãƒã‚¾ãƒ³", "ãƒ•ã‚¡ã‚¤ãƒ³ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³",
    "ãƒ•ã‚¸ã‚­ã‚»ã‚­", "ãƒãƒã‚«ãƒãƒ•ã‚¯ã‚­ã‚¿ãƒ«", "ãƒãƒ¤ãƒãƒˆãƒƒãƒ—ã‚¬ãƒ³", "ãƒãƒ³ãƒãƒƒã‚¿ãƒ³ã‚«ãƒ•ã‚§", "ãƒŸãƒ›ãƒãƒ–ãƒ«ãƒœãƒ³",
    "ãƒ¡ã‚¤ã‚·ãƒ§ã‚¦ãƒ‰ãƒˆã‚¦", "ãƒ¡ã‚¸ãƒ­ã‚¢ãƒ«ãƒ€ãƒ³", "ãƒ¡ã‚¸ãƒ­ãƒ‰ãƒ¼ãƒ™ãƒ«", "ãƒ¡ã‚¸ãƒ­ãƒãƒƒã‚¯ã‚¤ãƒ¼ãƒ³", "ãƒ¤ã‚¨ãƒãƒ ãƒ†ã‚­",
    "ãƒ©ã‚¤ã‚¹ã‚·ãƒ£ãƒ¯ãƒ¼", "ãƒ©ã‚¤ãƒ³ã‚¯ãƒ©ãƒ•ãƒˆ", "ãƒãƒ«ã‚¼ãƒ³ã‚¹ã‚­ãƒ¼", "ã‚·ãƒªã‚¦ã‚¹ã‚·ãƒ³ãƒœãƒª"
}
normalized_umamusume = {unicodedata.normalize("NFKC", n).strip().lower() for n in umamusume_bloodlines}

# === è¡€çµ±ãƒã‚¸ã‚·ãƒ§ãƒ³ ===
def generate_position_labels():
    def dfs(pos, depth, max_depth):
        if depth > max_depth: return []
        result = [pos]
        result += dfs(pos + "çˆ¶", depth + 1, max_depth)
        result += dfs(pos + "æ¯", depth + 1, max_depth)
        return result
    return dfs("", 0, 5)[1:]
POSITION_LABELS = generate_position_labels()

# === é¦¬ãƒªãƒ³ã‚¯å–å¾— ===
def get_horse_links(race_id):
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    options = Options()
    options.add_argument('--headless')
    driver = webdriver.Chrome(service=Service(CHROMEDRIVER_PATH), options=options)
    driver.get(url)
    time.sleep(3)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    horse_links = {}
    tables = soup.find_all("table", class_="RaceTable01")
    for table in tables:
        for a in table.find_all("a", href=True):
            if "/horse/" in a["href"]:
                name = a.get_text(strip=True)
                full_url = "https://db.netkeiba.com" + a["href"]
                if len(name) >= 2 and name not in horse_links:
                    horse_links[name] = full_url
    return horse_links

# === è¡€çµ±å–å¾—ï¼†ç…§åˆ ===
def get_pedigree_with_positions(horse_url):
    horse_id = horse_url.rstrip("/").split("/")[-1]
    ped_url = f"https://db.netkeiba.com/horse/ped/{horse_id}/"
    res = requests.get(ped_url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")
    table = soup.find("table", class_="blood_table")
    if not table:
        return {}
    names = {}
    td_list = table.find_all("td")
    for i, td in enumerate(td_list[:len(POSITION_LABELS)]):
        label = POSITION_LABELS[i]
        a = td.find("a")
        if a and a.text.strip():
            names[label] = a.text.strip()
    return names

def match_umamusume(pedigree_dict):
    return [f"ã€{pos}ã€‘{name}" for pos, name in pedigree_dict.items()
            if unicodedata.normalize("NFKC", name).strip().lower() in normalized_umamusume]

def analyze_race(race_id):
    horse_links = get_horse_links(race_id)
    st.text(f"ğŸ å‡ºèµ°é¦¬æ•°: {len(horse_links)}é ­")

    result = []
    for name, link in horse_links.items():
        try:
            pedigree = get_pedigree_with_positions(link)
            matches = match_umamusume(pedigree)
            result.append({
                "é¦¬å": name,
                "è©²å½“è¡€çµ±æ•°": len(matches),
                "ã‚¦ãƒå¨˜è¡€çµ±": "\n".join(matches)
            })
            time.sleep(1.0)
        except Exception as e:
            result.append({
                "é¦¬å": name,
                "è©²å½“è¡€çµ±æ•°": "å–å¾—å¤±æ•—",
                "ã‚¦ãƒå¨˜è¡€çµ±": str(e)
            })
    return result

# === UI ===
st.title("ğŸ“…JRAé–‹å‚¬é¸æŠï¼‹ã‚¦ãƒå¨˜è¡€çµ±ç…§åˆï¼ˆç«¶é¦¬å ´ãƒ»12Rãƒœã‚¿ãƒ³å¯¾å¿œï¼‰")

schedule_df = pd.read_csv("jra_2025_keibabook_schedule.csv")
today = pd.Timestamp.today()
past_31 = today - pd.Timedelta(days=31)
schedule_df["æ—¥ä»˜"] = pd.to_datetime(schedule_df["å¹´"].astype(str) + schedule_df["æœˆæ—¥(æ›œæ—¥)"].str[:5], format="%Y%m/%d")
schedule_df = schedule_df[schedule_df["æ—¥ä»˜"].between(past_31, today)]

# ğŸ“… æ—¥ä»˜é¸æŠï¼ˆæœ€æ–°ãŒä¸Šï¼‰
dates = sorted(schedule_df["æ—¥ä»˜"].dt.strftime("%Y-%m-%d").unique(), reverse=True)
selected_date = st.selectbox("é–‹å‚¬æ—¥ã‚’é¸æŠ", dates)
data_filtered = schedule_df[schedule_df["æ—¥ä»˜"].dt.strftime("%Y-%m-%d") == selected_date]

# ğŸ‡ ç«¶é¦¬å ´é¸æŠï¼ˆãƒœã‚¿ãƒ³å½¢å¼ï¼‰
st.markdown("### ğŸŸï¸ ç«¶é¦¬å ´ã‚’é¸æŠ")
place_codes = {"æœ­å¹Œ": "01", "å‡½é¤¨": "02", "ç¦å³¶": "03", "æ–°æ½Ÿ": "04", "æ±äº¬": "05",
                "ä¸­å±±": "06", "ä¸­äº¬": "07", "äº¬éƒ½": "08", "é˜ªç¥": "09", "å°å€‰": "10"}
available_places = sorted(data_filtered["ç«¶é¦¬å ´"].unique())
cols = st.columns(5)
if "place" not in st.session_state:
    st.session_state.place = None
for i, p in enumerate(available_places):
    if cols[i % 5].button(p):
        st.session_state.place = p
place = st.session_state.place
if not place:
    st.stop()

# ğŸ ãƒ¬ãƒ¼ã‚¹ç•ªå·ãƒœã‚¿ãƒ³
st.markdown("### ğŸ ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’é¸æŠ")
cols = st.columns(6)
if "race_num_int" not in st.session_state:
    st.session_state.race_num_int = None
for i in range(12):
    if cols[i % 6].button(f"{i+1}R"):
        st.session_state.race_num_int = i + 1
race_num_int = st.session_state.race_num_int
if not race_num_int:
    st.stop()

selected_row = data_filtered[data_filtered["ç«¶é¦¬å ´"] == place].iloc[0]
jj = place_codes[place]
kk = f"{int(selected_row['é–‹å‚¬å›']):02d}"
dd = f"{int(selected_row['æ—¥ç›®']):02d}"
race_id = f"{selected_row['å¹´']}{jj}{kk}{dd}{race_num_int:02d}"
st.markdown(f"ğŸ”¢ **race_id**: `{race_id}`")

# å®Ÿè¡Œãƒœã‚¿ãƒ³
if st.button("ğŸ§¬ è¡€çµ±ç…§åˆã‚’é–‹å§‹"):
    with st.spinner("ç…§åˆä¸­..."):
        results = analyze_race(race_id)
        st.success("ç…§åˆå®Œäº†ï¼")

        if not results:
            st.warning("å‡ºèµ°é¦¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.markdown("### ğŸ§¬ ã‚¦ãƒå¨˜è¡€çµ±ç…§åˆçµæœ")
            for idx, row in enumerate(results):
                st.markdown(f"""
<div style='font-size:20px; font-weight:bold;'>{idx+1}. {row['é¦¬å']}</div>

è©²å½“è¡€çµ±æ•°ï¼š{row['è©²å½“è¡€çµ±æ•°']}  
{row['ã‚¦ãƒå¨˜è¡€çµ±']}
""", unsafe_allow_html=True)
                if idx < len(results) - 1:
                    st.markdown("<hr style='border:1px solid #ccc;'>", unsafe_allow_html=True)