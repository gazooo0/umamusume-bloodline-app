import pandas as pd
import datetime
import requests
import re
import gspread
import unicodedata
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup

SCHEDULE_CSV_PATH = 'jra_2025_keibabook_schedule.csv'
UMAMUSUME_BLOODLINE_CSV = 'umamusume.csv'
SPREADSHEET_ID = '1wMkpbOvqveVBkJSR85mpZcnKThYSEmusmsl710SaRKw'
SHEET_NAME = 'cache'

HEADERS = {"User-Agent": "Mozilla/5.0"}

# Google Sheets æ¥ç¶š
def connect_to_gspread():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    credentials = ServiceAccountCredentials.from_json_keyfile_name('service_account.json', scope)
    gc = gspread.authorize(credentials)
    return gc.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)

# é–‹å‚¬åœ°ã‚³ãƒ¼ãƒ‰å–å¾—
def get_place_code(place_name):
    place_dict = {
        'æœ­å¹Œ': '01', 'å‡½é¤¨': '02', 'ç¦å³¶': '03', 'æ–°æ½Ÿ': '04',
        'æ±äº¬': '05', 'ä¸­å±±': '06', 'ä¸­äº¬': '07', 'äº¬éƒ½': '08', 'é˜ªç¥': '09', 'å°å€‰': '10'
    }
    return place_dict.get(place_name, '00')

# é–‹å‚¬æ—¥ã‹ã‚‰7æ—¥é–“åˆ†ã®race_idç”Ÿæˆ
def generate_future_race_ids(base_date):
    df = pd.read_csv(SCHEDULE_CSV_PATH)

    print("ğŸ“ CSVèª­ã¿è¾¼ã¿æˆåŠŸã€‚å…ˆé ­5è¡Œ:\n", df.head())

    # æ—¥ä»˜åˆ—ã‚’æ§‹ç¯‰ã—ã¦å¤‰æ›
    df['æ—¥ä»˜'] = pd.to_datetime(df['å¹´'].astype(str) + '/' + df['æœˆæ—¥(æ›œæ—¥)'].str.extract(r'(\d+/\d+)')[0], errors='coerce')
    print("ğŸ“† æ—¥ä»˜å¤‰æ›å¾Œã®å…ˆé ­:\n", df[['å¹´', 'æœˆæ—¥(æ›œæ—¥)', 'æ—¥ä»˜']].head())

    df = df[df['æ—¥ä»˜'].notnull()]

    # å‹ã‚’æƒãˆã¦æ¯”è¼ƒ
    start_date = pd.to_datetime(base_date)
    end_date = pd.to_datetime(base_date + datetime.timedelta(days=6))
    df = df[df['æ—¥ä»˜'].between(start_date, end_date)]

    race_ids = []
    for _, row in df.iterrows():
        year = f"{int(row['å¹´']):04d}"
        place_code = get_place_code(row['ç«¶é¦¬å ´'])
        kai = f"{int(row['é–‹å‚¬å›']):02d}"
        nichi = f"{int(row['æ—¥ç›®']):02d}"
        for race_num in range(1, 13):
            num = f"{race_num:02d}"
            # YYYYJJKKDDNN å½¢å¼ã®12æ¡
            race_id = f"{year}{place_code}{kai}{nichi}{num}"
            race_ids.append(race_id)

    print(f"ğŸ“… å¯¾è±¡race_idæ•°: {len(race_ids)}")
    return race_ids

# netkeibaã‹ã‚‰å‡ºèµ°é¦¬ã®è¡€çµ±ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã‚’å–å¾—
def get_horse_links(race_id):
    url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    res = requests.get(url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")

    horse_links = {}
    tables = soup.find_all("table", class_="RaceTable01")
    for table in tables:
        for a in table.find_all("a", href=True):
            if "/horse/" in a["href"]:
                name = a.get_text(strip=True)
                full_url = "https://db.netkeiba.com" + a["href"]
                if len(name) >= 2 and name not in horse_links:
                    horse_links[name] = full_url
    return horse_links

# è¡€çµ±ãƒšãƒ¼ã‚¸ã‹ã‚‰5ä»£è¡€çµ±ã‚’å–å¾—
def get_pedigree_with_positions(horse_url, position_labels):
    horse_id = horse_url.rstrip("/").split("/")[-1]
    ped_url = f"https://db.netkeiba.com/horse/ped/{horse_id}/"
    res = requests.get(ped_url, headers=HEADERS)
    res.encoding = "EUC-JP"
    soup = BeautifulSoup(res.text, "html.parser")

    table = soup.find("table", class_="blood_table")
    if not table:
        return {}

    names = {}
    td_list = table.find_all("td")
    for i, td in enumerate(td_list[:len(position_labels)]):
        label = position_labels[i]
        a = td.find("a")
        if a and a.text.strip():
            names[label] = a.text.strip()
    return names

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    today = datetime.date.today()
    print(f"ğŸ” å®Ÿè¡Œæ—¥: {today}")

    race_ids = generate_future_race_ids(today)
    print(f"ğŸ“Œ å‡¦ç†å¯¾è±¡ã®race_id: {race_ids[:5]} ...")

    bloodline_df = pd.read_csv(UMAMUSUME_BLOODLINE_CSV)
    bloodline_keywords = set(bloodline_df['kettou'].dropna().tolist())
    print(f"ğŸ§¬ ç…§åˆå¯¾è±¡ã‚¦ãƒå¨˜è¡€çµ±æ•°: {len(bloodline_keywords)}")

    ws = connect_to_gspread()
    print("âœ… Google Sheets æ¥ç¶šæˆåŠŸ")

    for race_id in race_ids:
        print(f"ğŸ” å‡¦ç†ä¸­: {race_id}")
        horse_links = get_horse_links(race_id)
        print(f"ğŸ å‡ºèµ°é¦¬æ•°ï¼ˆè¡€çµ±ãƒªãƒ³ã‚¯å–å¾—ï¼‰: {len(horse_links)}")

        for link in horse_links:
            horse_name = link
            horse_url = horse_links[link]
        try:
            names_dict = get_pedigree_with_positions(horse_url, position_labels=[
                "çˆ¶", "æ¯", "æ¯çˆ¶", "çˆ¶æ¯", "çˆ¶çˆ¶", "æ¯æ¯", "æ¯æ¯çˆ¶", "æ¯çˆ¶æ¯"
            ])
        except Exception as e:
            print(f"âš ï¸ è¡€çµ±å–å¾—ã‚¨ãƒ©ãƒ¼: {horse_url} â†’ {e}")
            continue

        matches = [name for name in names_dict.values() if name in bloodline_keywords]
        if matches:
            row = [horse_name, len(matches), ', '.join(matches), race_id]
            ws.append_row(row)
            print(f"âœ… ç™»éŒ²: {row}")

if __name__ == '__main__':
    main()
