import os
import datetime
import pandas as pd
import requests
from bs4 import BeautifulSoup
import gspread
from google.oauth2.service_account import Credentials

# ===============================
# è¨­å®š
# ===============================
SCHEDULE_CSV_PATH = "jra_2025_keibabook_schedule.csv"
UMAMUSUME_CSV_PATH = "umamusume.csv"
GOOGLE_SHEET_NAME = "umamusume_cache"
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
SERVICE_ACCOUNT_FILE = 'service_account.json'

# ===============================
# Google Sheets ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—
# ===============================
def get_gspread_client():
    credentials = Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE,
        scopes=SCOPES
    )
    gc = gspread.authorize(credentials)
    return gc

# ===============================
# race_idç”Ÿæˆï¼ˆæœªæ¥7æ—¥é–“ï¼‰
# ===============================
def generate_future_race_ids(base_date):
    df = pd.read_csv(SCHEDULE_CSV_PATH)
    df['æ—¥ä»˜'] = pd.to_datetime(df['æœˆæ—¥(æ›œæ—¥)'], format='%mæœˆ%dæ—¥', errors='coerce')
    df['æ—¥ä»˜'] = df['æ—¥ä»˜'].apply(lambda d: d.replace(year=base_date.year) if pd.notnull(d) else d)
    df = df[df['æ—¥ä»˜'].notnull()]
    df = df[df['æ—¥ä»˜'].between(base_date, base_date + datetime.timedelta(days=6))]

    race_ids = []
    for _, row in df.iterrows():
        date_str = row['æ—¥ä»˜'].strftime('%Y%m%d')
        place_code = get_place_code(row['ç«¶é¦¬å ´'])
        kai = f"{int(row['é–‹å‚¬å›']):02d}"
        nichi = f"{int(row['æ—¥ç›®']):02d}"
        for race_num in range(1, 13):
            num = f"{race_num:02d}"
            race_id = f"{date_str}{place_code}{kai}{nichi}{num}"
            race_ids.append(race_id)
    return race_ids

def get_place_code(place_name):
    place_dict = {
        "æœ­å¹Œ": "01", "å‡½é¤¨": "02", "ç¦å³¶": "03", "æ–°æ½Ÿ": "04",
        "æ±äº¬": "05", "ä¸­å±±": "06", "ä¸­äº¬": "07", "äº¬éƒ½": "08",
        "é˜ªç¥": "09", "å°å€‰": "10"
    }
    return place_dict.get(place_name, "00")

# ===============================
# å‡ºèµ°é¦¬ã¨è¡€çµ±æƒ…å ±ã‚’å–å¾—
# ===============================
def scrape_pedigree_info(race_id):
    url = f"https://db.netkeiba.com/race/{race_id}/"
    res = requests.get(url)
    res.encoding = res.apparent_encoding
    soup = BeautifulSoup(res.text, "html.parser")
    horse_links = soup.select("td.horse a")

    horses = []
    for a in horse_links:
        name = a.text.strip()
        href = a.get("href")
        if not href:
            continue
        horse_id = href.split("/")[-2]
        horse_url = f"https://db.netkeiba.com/horse/ped/{horse_id}/"
        horses.append({
            "name": name,
            "url": horse_url
        })
    return horses

# ===============================
# è¡€çµ±ç…§åˆï¼ˆã‚¦ãƒå¨˜ã¨æ¯”è¼ƒï¼‰
# ===============================
def check_umamusume_bloodline(horse):
    res = requests.get(horse["url"])
    res.encoding = res.apparent_encoding
    soup = BeautifulSoup(res.text, "html.parser")
    table = soup.select_one("table.db_parent_table")
    if not table:
        return {"è©²å½“æ•°": 0, "è©²å½“ç®‡æ‰€": "", "name": horse["name"]}

    with open(UMAMUSUME_CSV_PATH, encoding='utf-8') as f:
        umamusume_list = [line.strip() for line in f if line.strip()]

    matches = []
    for td in table.select("td"):
        if td.text.strip() in umamusume_list:
            matches.append(f"<div>{td.text.strip()}</div>")

    return {
        "name": horse["name"],
        "è©²å½“æ•°": len(matches),
        "è©²å½“ç®‡æ‰€": "".join(matches)
    }

# ===============================
# Google Sheetsã¸ä¿å­˜
# ===============================
def save_to_sheets(results):
    gc = get_gspread_client()
    sh = gc.open(GOOGLE_SHEET_NAME)
    try:
        worksheet = sh.worksheet("cache")
    except gspread.exceptions.WorksheetNotFound:
        worksheet = sh.add_worksheet(title="cache", rows="1000", cols="10")

    existing = worksheet.get_all_values()
    headers = ["é¦¬å", "è©²å½“æ•°", "è©²å½“ç®‡æ‰€", "race_id"]
    if not existing:
        worksheet.append_row(headers)

    for row in results:
        worksheet.append_row([
            row["é¦¬å"],
            row["è©²å½“æ•°"],
            row["è©²å½“ç®‡æ‰€"],
            row["race_id"]
        ])

# ===============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===============================
def main():
    today = datetime.date.today()
    race_ids = generate_future_race_ids(today)
    for race_id in race_ids:
        print(f"ğŸ” {race_id}")
        try:
            horses = scrape_pedigree_info(race_id)
            results = []
            for horse in horses:
                result = check_umamusume_bloodline(horse)
                if result["è©²å½“æ•°"] > 0:
                    results.append({
                        "é¦¬å": horse["name"],
                        "è©²å½“æ•°": result["è©²å½“æ•°"],
                        "è©²å½“ç®‡æ‰€": result["è©²å½“ç®‡æ‰€"],
                        "race_id": race_id
                    })
            if results:
                save_to_sheets(results)
        except Exception as e:
            print(f"âŒ Error: {e}")

if __name__ == "__main__":
    main()
